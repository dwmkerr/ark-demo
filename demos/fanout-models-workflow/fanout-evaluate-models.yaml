apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: fanout-evaluate-models
  annotations:
    workflows.argoproj.io/description: 'Query all ARK models in parallel and evaluate responses'
spec:
  entrypoint: main
  serviceAccountName: argo-workflow
  arguments:
    parameters:
      - name: question
        value: "Summarise how argo fanout workflows work"
      - name: evaluator-agent
        value: "sample-agent"
      - name: evaluation-prompt
        value: "Look at the responses of each model's query and summarise the results into a table. Include model name, response quality, and token usage (input/output/total) for each model. Then make a suggestion of which model did the best work and provide thoughts on token efficiency vs response quality."
  templates:
    - name: main
      steps:
        - - name: list-models
            template: kubectl-get-models

        - - name: query-model
            template: query-and-wait
            arguments:
              parameters:
                - name: model-name
                  value: "{{item}}"
                - name: question
                  value: "{{workflow.parameters.question}}"
            withParam: "{{steps.list-models.outputs.parameters.model-names}}"
            continueOn:
              failed: true  # Continue even if some models fail

        - - name: evaluate-responses
            template: evaluate-all
            arguments:
              parameters:
                - name: evaluator-agent
                  value: "{{workflow.parameters.evaluator-agent}}"
                - name: evaluation-prompt
                  value: "{{workflow.parameters.evaluation-prompt}}"

      outputs:
        parameters:
          - name: total-tokens
            valueFrom:
              parameter: "{{steps.evaluate-responses.outputs.parameters.total-tokens}}"
            description: "Total tokens used across all models"
          - name: recommended-model
            valueFrom:
              parameter: "{{steps.evaluate-responses.outputs.parameters.recommended-model}}"
            description: "The recommended model name"
          - name: summary
            valueFrom:
              parameter: "{{steps.evaluate-responses.outputs.parameters.summary}}"
            description: "Brief summary with tokens, model, and recommendation"
          - name: full-evaluation
            valueFrom:
              parameter: "{{steps.evaluate-responses.outputs.parameters.evaluation}}"
            description: "Complete evaluation details"

    - name: kubectl-get-models
      script:
        image: alpine/k8s:1.28.13
        command: [sh]
        source: |
          echo "Fetching ARK models..." >&2
          # Get model names as JSON array directly
          kubectl get models -o json | jq '[.items[].metadata.name]' > /tmp/model-names.json
          echo "Found $(kubectl get models -o json | jq '.items | length') models" >&2
          cat /tmp/model-names.json
      outputs:
        parameters:
          - name: model-names
            valueFrom:
              path: /tmp/model-names.json
            description: "Array of model names for fanout"

    - name: query-and-wait
      inputs:
        parameters:
          - name: model-name
          - name: question
      script:
        image: alpine/k8s:1.28.13
        command: [sh]
        source: |
          echo "Creating Query for model: {{inputs.parameters.model-name}}"
          QUERY_NAME="workflow-query-{{inputs.parameters.model-name}}-$(date +%s)"

          # Create the Query resource
          cat <<EOF | kubectl apply -f -
          apiVersion: ark.mckinsey.com/v1alpha1
          kind: Query
          metadata:
            name: $QUERY_NAME
          spec:
            input: "{{inputs.parameters.question}}"
            targets:
            - name: "{{inputs.parameters.model-name}}"
              type: model
            timeout: 5m
            ttl: 1h
          EOF

          # Wait for Query to complete (up to 5 minutes)
          echo "Waiting for Query $QUERY_NAME to complete..."
          for i in $(seq 1 60); do
            PHASE=$(kubectl get query $QUERY_NAME -o jsonpath='{.status.phase}' 2>/dev/null || echo "pending")
            if [ "$PHASE" = "done" ] || [ "$PHASE" = "error" ]; then
              echo "Query completed with phase: $PHASE"
              break
            fi
            echo "Query phase: $PHASE (attempt $i/60)"
            sleep 5
          done

          # Get the full Query object
          echo "Getting Query result..."
          kubectl get query $QUERY_NAME -o json > /tmp/query.json

          # Extract key info for logging
          PHASE=$(kubectl get query $QUERY_NAME -o jsonpath='{.status.phase}' 2>/dev/null || echo "error")
          if [ "$PHASE" = "error" ]; then
            echo "Query failed for model {{inputs.parameters.model-name}}"
            ERROR=$(kubectl get query $QUERY_NAME -o jsonpath='{.status.error}' 2>/dev/null || echo "Unknown error")
            echo "Error: $ERROR"
            # Exit with error to mark step as failed
            exit 1
          else
            RESPONSE=$(kubectl get query $QUERY_NAME -o json | jq -r '.status.responses[0].content // "No response"' | head -c 200)
            echo "Model: {{inputs.parameters.model-name}}"
            echo "Phase: $PHASE"
            echo "Response preview: $RESPONSE..."
          fi
      outputs:
        parameters:
          - name: query-json
            valueFrom:
              path: /tmp/query.json
            description: "Full Query object with response and token counts"

    - name: evaluate-all
      inputs:
        parameters:
          - name: evaluator-agent
          - name: evaluation-prompt
      script:
        image: alpine/k8s:1.28.13
        command: [sh]
        source: |
          echo "Preparing evaluation data..."

          # Collect all workflow queries from this run
          TIMESTAMP=$(date +%s)
          kubectl get queries -o json | jq "[.items[] | select(.metadata.name | startswith(\"workflow-query-\"))]" > /tmp/all-queries.json

          echo "Found $(cat /tmp/all-queries.json | jq 'length') workflow queries"

          # Create a simpler format for the evaluation
          cat > /tmp/format-queries.jq << 'JQSCRIPT'
          "Here are the Query results from all models:\n\n" +
          (
            map(
              "Model: " + .spec.targets[0].name + "\n" +
              "Status: " + .status.phase + "\n" +
              if .status.phase == "done" then
                "Response: " + (.status.responses[0].content // "No response") + "\n" +
                "Token Usage:\n" +
                "  Prompt Tokens: " + (.status.tokenUsage.promptTokens // 0 | tostring) + "\n" +
                "  Completion Tokens: " + (.status.tokenUsage.completionTokens // 0 | tostring) + "\n" +
                "  Total Tokens: " + (.status.tokenUsage.totalTokens // 0 | tostring) + "\n"
              else
                "Error: " + (.status.error // "Model unavailable or API error") + "\n"
              end +
              "\n---\n"
            ) | join("")
          )
          JQSCRIPT

          cat /tmp/all-queries.json | jq -r -f /tmp/format-queries.jq > /tmp/model-results.txt

          # Append the evaluation prompt
          echo "" >> /tmp/model-results.txt
          echo "{{inputs.parameters.evaluation-prompt}}" >> /tmp/model-results.txt

          # Show what we're sending
          echo "=== EVALUATION INPUT ==="
          head -50 /tmp/model-results.txt
          echo "..."

          echo "Creating evaluation Query..."
          EVAL_QUERY="workflow-evaluation-$TIMESTAMP"

          # Create Query YAML with proper escaping
          cat > /tmp/eval-query.yaml << EOF
          apiVersion: ark.mckinsey.com/v1alpha1
          kind: Query
          metadata:
            name: $EVAL_QUERY
          spec:
            input: |
          EOF

          # Add the input as indented text
          cat /tmp/model-results.txt | sed 's/^/      /' >> /tmp/eval-query.yaml

          # Add the rest of the spec
          cat >> /tmp/eval-query.yaml << EOF
            targets:
            - name: "{{inputs.parameters.evaluator-agent}}"
              type: agent
            timeout: 5m
            ttl: 1h
          EOF

          kubectl apply -f /tmp/eval-query.yaml

          # Wait for evaluation to complete
          echo "Waiting for evaluation to complete..."
          for i in $(seq 1 60); do
            PHASE=$(kubectl get query $EVAL_QUERY -o jsonpath='{.status.phase}' 2>/dev/null || echo "pending")
            if [ "$PHASE" = "done" ] || [ "$PHASE" = "error" ]; then
              echo "Evaluation completed with phase: $PHASE"
              break
            fi
            echo "Evaluation phase: $PHASE (attempt $i/60)"
            sleep 5
          done

          # Get evaluation result
          kubectl get query $EVAL_QUERY -o json | jq -r '.status.responses[0].content // .status.error // "No evaluation response"' > /tmp/evaluation.txt

          echo "=== EVALUATION RESULT ==="
          cat /tmp/evaluation.txt

          # Extract summary information
          echo "Calculating total tokens used..."
          TOTAL_TOKENS=$(cat /tmp/all-queries.json | jq '[.[] | .status.tokenUsage.totalTokens // 0] | add')
          echo "Total tokens across all models: $TOTAL_TOKENS"

          # Try to extract recommended model from evaluation
          echo "Extracting recommendation..."
          RECOMMENDED_MODEL=$(cat /tmp/evaluation.txt | grep -i "recommend\|best model\|suggested.*model" | head -1 | grep -oE "(claude-[^[:space:]]+|gemini-[^[:space:]]+|default|gpt-[^[:space:]]+)" | head -1 || echo "")

          # If no model found in recommendation, try to find it in the evaluation
          if [ -z "$RECOMMENDED_MODEL" ]; then
            # Look for model names after "recommend" or "best"
            RECOMMENDED_MODEL=$(cat /tmp/evaluation.txt | grep -A2 -i "recommend\|best model" | grep -oE "(claude-[^[:space:]]+|gemini-[^[:space:]]+|default|gpt-[^[:space:]]+)" | head -1 || echo "Unable to determine")
          fi

          # Get the recommendation sentence
          RECOMMENDATION=$(cat /tmp/evaluation.txt | grep -i "recommend" | head -1 | cut -d':' -f2- | sed 's/^ *//' || echo "")
          if [ -z "$RECOMMENDATION" ]; then
            RECOMMENDATION="See full evaluation for detailed comparison"
          fi

          # Create concise summary for output
          echo "Creating summary..."
          cat > /tmp/summary.txt << EOF
          Total Tokens: $TOTAL_TOKENS
          Recommended Model: $RECOMMENDED_MODEL
          Recommendation: $RECOMMENDATION
          EOF

          echo "$TOTAL_TOKENS" > /tmp/total-tokens.txt
          echo "$RECOMMENDED_MODEL" > /tmp/recommended-model.txt

      outputs:
        parameters:
          - name: evaluation
            valueFrom:
              path: /tmp/evaluation.txt
            description: "Full agent evaluation of all model responses"
          - name: total-tokens
            valueFrom:
              path: /tmp/total-tokens.txt
            description: "Total tokens used across all models"
          - name: recommended-model
            valueFrom:
              path: /tmp/recommended-model.txt
            description: "The recommended model based on evaluation"
          - name: summary
            valueFrom:
              path: /tmp/summary.txt
            description: "Brief summary with total tokens, recommended model, and recommendation"
